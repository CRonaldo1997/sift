import cv2
from PIL import Image
import numpy as np
from time import time
import glob
import sys
sys.path.append("./ocr")
from ocr.model import predict

def sift_kp(image):
    gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
    sift = cv2.xfeatures2d_SIFT.create()
    kp,des = sift.detectAndCompute(image,None)
    kp_image = cv2.drawKeypoints(gray_image,kp,None)
    return kp_image,kp,des


def get_good_match(des1,des2):
    bf = cv2.BFMatcher()
    matches = bf.knnMatch(des1, des2, k=2)
    good = []
    for m, n in matches:
        if m.distance < 0.75 * n.distance:
            good.append(m)
    return good


def siftImageAlignment(img1,img2):
   _,kp1,des1 = sift_kp(img1)
   _,kp2,des2 = sift_kp(img2)
   goodMatch = get_good_match(des1,des2)
   if len(goodMatch) > 4:
       ptsA= np.float32([kp1[m.queryIdx].pt for m in goodMatch]).reshape(-1, 1, 2)
       ptsB = np.float32([kp2[m.trainIdx].pt for m in goodMatch]).reshape(-1, 1, 2)
       ransacReprojThreshold = 4
       H, status =cv2.findHomography(ptsA,ptsB,cv2.RANSAC,ransacReprojThreshold);
       imgOut = cv2.warpPerspective(img2, H, (img1.shape[1],img1.shape[0]),flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)
   return imgOut,H,status


def get_ocr_result(crop_region,gray):
    str_list = []
    for idx in idx_list:
        roi = crop_region[idx]
        crop_img_array = gray[roi[0]:roi[1], roi[2]:roi[3]]
        crop_img = Image.fromarray(crop_img_array)

        text = predict(crop_img)+' '
        str_list.append(text)
    return ''.join(str_list).strip()

img1 = cv2.imread('./template/idcard.jpg')
#img2 = cv2.imread('./testSet/.jpg')
img_path = './testSet/'
img_path_list = glob.glob(img_path+'*.*')

print('template shape ',str(img1.shape))
# print(img2.shape)

chi_region = [220,328,60,600]
eng_region = [330,420,60,1400]
dob_region = [698,790,586,1090]
gender_region = [694,790,1200,1410]
cardno_region = [1075,1200,1250,1900]

for img_path in img_path_list:
    img2 = cv2.imread(img_path)
    print('current img is ',img_path)
    start = time()
    result, _, _ = siftImageAlignment(img1,img2)
    print('result shape is, ',result.shape)
    # cv2.imwrite('./resultImg/result.jpg',result)
    # cv2.imshow('result',result)
    # cv2.waitKey(0)

    chi_name_img = result[chi_region[0]:chi_region[1],chi_region[2]:chi_region[3]]
    # cv2.imshow('chi_name_img',chi_name_img)
    # cv2.waitKey(0)

    eng_name_img = result[eng_region[0]:eng_region[1],eng_region[2]:eng_region[3]]
    # cv2.imshow('eng_name_img',eng_name_img)
    # cv2.waitKey(0)

    dob_img = result[dob_region[0]:dob_region[1],dob_region[2]:dob_region[3]]
    # cv2.imshow('dob_name_img',dob_img)
    # cv2.waitKey(0)

    gender_img = result[gender_region[0]:gender_region[1],gender_region[2]:gender_region[3]]
    # cv2.imshow('gender_region',gender_img)
    # cv2.waitKey(0)

    cardno_img = result[cardno_region[0]:cardno_region[1],cardno_region[2]:cardno_region[3]]
    # cv2.imshow('cardno_region',cardno_img)
    # cv2.waitKey(0)

    end = time()
    print('time cost is: ',end - start)

    #get ocr result
    chi_name_raw = predict(Image.fromarray(chi_name_img))
    eng_name_raw = predict(Image.fromarray(eng_name_img))
    dob_raw = predict(Image.fromarray(dob_img))
    gender_raw = predict(Image.fromarray(gender_img))
    cardno_raw = predict(Image.fromarray(cardno_img))

    print(chi_name_raw,eng_name_raw,dob_raw,gender_raw,cardno_raw)
